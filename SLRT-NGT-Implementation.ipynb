{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a345a3a2-0d77-4801-99f7-a2c2480b525a",
   "metadata": {},
   "source": [
    "The idea now:\n",
    "Since their implementation of the model requires packages only available for a linux engine, I will have to retrofit their model to work on my computer.\n",
    "\n",
    "For the SingleStream network I need:\n",
    "- Visual Encoder:\n",
    "    - S3D Backbone 1-4 blocks, outputs S3D features (T/4x843) --> these features are used for the head network                    CHECK\n",
    "    - Head network, outputs Gloss representations in high dim space (T/4x512) --> These are the features sent forward to the S2T  CHECK\n",
    "        - Linear/BN/ReLU                                                                                                          CHECK\n",
    "        - Temporal Cov Block                                                                                                      CHECK\n",
    "    - Linear classifier, outputs Gloss logits (T/4xK)                                                                             CHECK\n",
    "    - Softmax, outputs gloss probabilities (T/4xK)                                                                                CHECK\n",
    "    - and then the CTC (connectionist temporal classification) loss and the CTC Decoder (which outputs Gloss Predictions)\n",
    "\n",
    "- Pretraining of the Visual Encoder (not necessary since we will be using their pretrained weights).\n",
    "\n",
    "- V-L Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7289d8f-458f-44d4-b876-03a92331868f",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c413f1ad-b54a-4685-a105-c24974c3e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "from sys import platform\n",
    "from logging import Logger\n",
    "from typing import Callable, Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torchinfo import summary\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import yaml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def neq_load_customized(model, pretrained_dict, verbose=False):\n",
    "    ''' load pre-trained model in a not-equal way,\n",
    "    when new model has been partially modified '''\n",
    "    model_dict = model.state_dict()\n",
    "    tmp = {}\n",
    "    if verbose:\n",
    "        print(list(model_dict.keys()))\n",
    "        print('\\n=======Check Weights Loading======')\n",
    "        print('Weights not used from pretrained file:')\n",
    "    for k, v in pretrained_dict.items():\n",
    "        if k in model_dict and model_dict[k].shape==v.shape:\n",
    "            tmp[k] = v\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(k)\n",
    "    if verbose:\n",
    "        print('---------------------------')\n",
    "        print('Weights not loaded into new model:')\n",
    "        for k, v in model_dict.items():\n",
    "            if k not in pretrained_dict:\n",
    "                print(k)\n",
    "            elif model_dict[k].shape != pretrained_dict[k].shape:\n",
    "                print(k, 'shape mis-matched, not loaded')\n",
    "        print('===================================\\n')\n",
    "\n",
    "    del pretrained_dict\n",
    "    model_dict.update(tmp)\n",
    "    del tmp\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa12888-bc1a-4558-9781-00bc0d5d5c32",
   "metadata": {},
   "source": [
    "## S3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dc331-4a3f-4480-9d0a-a09373423846",
   "metadata": {},
   "source": [
    "#### S3D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ec4f9e-5f8d-4992-97f5-5354a6aba873",
   "metadata": {},
   "outputs": [],
   "source": [
    "### S3D Model architecture\n",
    "\n",
    "class S3Dsup(nn.Module):\n",
    "    def __init__(self, in_channels, num_class, use_block, stride):\n",
    "        super(S3Dsup, self).__init__()\n",
    "        base_seq = []\n",
    "        if use_block>=1:\n",
    "            base_seq += [\n",
    "                SepConv3d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            ]\n",
    "        if use_block>=2:\n",
    "            base_seq += [\n",
    "                nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(\n",
    "                    1, 2, 2), padding=(0, 1, 1)),                        # 1\n",
    "                BasicConv3d(64, 64, kernel_size=1, stride=1),            # 2\n",
    "                SepConv3d(64, 192, kernel_size=3, stride=1, padding=1),  # 3\n",
    "            ]\n",
    "        if use_block>=3:\n",
    "            base_seq += [\n",
    "                nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(\n",
    "                    1, 2, 2), padding=(0, 1, 1)),                        # 4\n",
    "                Mixed_3b(),                                              # 5\n",
    "                Mixed_3c(),                                              # 6\n",
    "            ]\n",
    "        if use_block>=4:\n",
    "            base_seq += [\n",
    "                nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(\n",
    "                    2, 2, 2), padding=(1, 1, 1)),                        # 7\n",
    "                Mixed_4b(),                                              # 8\n",
    "                Mixed_4c(),                                              # 9\n",
    "                Mixed_4d(),                                              # 10\n",
    "                Mixed_4e(),                                              # 11\n",
    "                Mixed_4f(),                                              # 12\n",
    "            ]\n",
    "        if use_block>=5:\n",
    "            base_seq += [\n",
    "                nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(\n",
    "                    stride, 2, 2), padding=(0 if stride==2 else 1, 0, 0)),\n",
    "                Mixed_5b(),\n",
    "                Mixed_5c(), #15\n",
    "            ]\n",
    "        self.base_num_layers = len(base_seq)\n",
    "        self.base = nn.Sequential(*base_seq)\n",
    "        #self.fc = nn.Sequential(nn.Conv3d(BLOCK2SIZE[use_block], num_class, kernel_size=1, stride=1, bias=True)) \n",
    "        # Took the standard fc from S3D class pytorch, allows the model to load the weights, so we assume it's the right one\n",
    "        # 1024 for kinetics, 832 for gloss since different blocksize\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        y = F.avg_pool3d(y, (2, y.size(3), y.size(4)), stride=1)\n",
    "        #y = self.fc(y)\n",
    "        y = y.view(y.size(0), y.size(1), y.size(2))\n",
    "        logits = torch.mean(y, 2)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BasicConv3d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(BasicConv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class SepConv3d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(SepConv3d, self).__init__()\n",
    "        self.conv_s = nn.Conv3d(in_planes, out_planes, kernel_size=(1,kernel_size,kernel_size), stride=(1,stride,stride), padding=(0,padding,padding), bias=False)\n",
    "        self.bn_s = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
    "        self.relu_s = nn.ReLU()\n",
    "\n",
    "        self.conv_t = nn.Conv3d(out_planes, out_planes, kernel_size=(kernel_size,1,1), stride=(stride,1,1), padding=(padding,0,0), bias=False)\n",
    "        self.bn_t = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
    "        self.relu_t = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_s(x)\n",
    "        x = self.bn_s(x)\n",
    "        x = self.relu_s(x)\n",
    "\n",
    "        x = self.conv_t(x)\n",
    "        x = self.bn_t(x)\n",
    "        x = self.relu_t(x)\n",
    "        return x\n",
    "\n",
    "class Mixed_3b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_3b, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(192, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(192, 96, kernel_size=1, stride=1),\n",
    "            SepConv3d(96, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(192, 16, kernel_size=1, stride=1),\n",
    "            SepConv3d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(192, 32, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_3c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_3c, self).__init__()\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(256, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(256, 128, kernel_size=1, stride=1),\n",
    "            SepConv3d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(256, 32, kernel_size=1, stride=1),\n",
    "            SepConv3d(32, 96, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(256, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_4b, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(480, 192, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(480, 96, kernel_size=1, stride=1),\n",
    "            SepConv3d(96, 208, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(480, 16, kernel_size=1, stride=1),\n",
    "            SepConv3d(16, 48, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(480, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_4c, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(512, 160, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(512, 112, kernel_size=1, stride=1),\n",
    "            SepConv3d(112, 224, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(512, 24, kernel_size=1, stride=1),\n",
    "            SepConv3d(24, 64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_4d, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(512, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(512, 128, kernel_size=1, stride=1),\n",
    "            SepConv3d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(512, 24, kernel_size=1, stride=1),\n",
    "            SepConv3d(24, 64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4e(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_4e, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(512, 112, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(512, 144, kernel_size=1, stride=1),\n",
    "            SepConv3d(144, 288, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(512, 32, kernel_size=1, stride=1),\n",
    "            SepConv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4f(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_4f, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(528, 256, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(528, 160, kernel_size=1, stride=1),\n",
    "            SepConv3d(160, 320, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(528, 32, kernel_size=1, stride=1),\n",
    "            SepConv3d(32, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(528, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_5b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_5b, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(832, 256, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(832, 160, kernel_size=1, stride=1),\n",
    "            SepConv3d(160, 320, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(832, 32, kernel_size=1, stride=1),\n",
    "            SepConv3d(32, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(832, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_5c(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mixed_5c, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv3d(832, 384, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv3d(832, 192, kernel_size=1, stride=1),\n",
    "            SepConv3d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv3d(832, 48, kernel_size=1, stride=1),\n",
    "            SepConv3d(48, 128, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
    "            BasicConv3d(832, 128, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff9fb9-2f20-4f5c-aa86-859c86c70038",
   "metadata": {},
   "source": [
    "#### Initialize S3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f9af8d-13ac-48bc-903e-d82292b147b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize S3D\n",
    "\n",
    "BLOCK2SIZE = {1:64, 2:192, 3:480, 4:832, 5:1024}\n",
    "\n",
    "class S3Ds(S3Dsup):\n",
    "    def __init__(self, num_class=400, in_channel=3, use_block=5, freeze_block=0, stride=2):  # 5 and 0 for kinetics, 4 and 1 for gloss\n",
    "        self.use_block = use_block\n",
    "        super(S3Ds, self).__init__(in_channels=in_channel, num_class=num_class, use_block=use_block, stride=stride)\n",
    "        self.freeze_block = freeze_block\n",
    "        self.END_POINT2BLOCK = {\n",
    "            0: 'block1',\n",
    "            3: 'block2',\n",
    "            6: 'block3',\n",
    "            12: 'block4',\n",
    "            15: 'block5',\n",
    "        }\n",
    "        self.BLOCK2END_POINT = {blk:ep for ep, blk in self.END_POINT2BLOCK.items()}\n",
    "\n",
    "        self.frozen_modules = []\n",
    "        self.use_block = use_block\n",
    "\n",
    "        if freeze_block>0:\n",
    "            for i in range(0, self.base_num_layers): #base  0,1,2,...,self.BLOCK2END_POINT[blk]\n",
    "                module_name = 'base.{}'.format(i)\n",
    "                submodule = self.base[i]\n",
    "                assert submodule != None, module_name\n",
    "                if i <= self.BLOCK2END_POINT['block{}'.format(freeze_block)]:\n",
    "                    self.frozen_modules.append(submodule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4b2e2-5ff5-4683-97a8-b5049b84b336",
   "metadata": {},
   "source": [
    "#### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737e58dc-cf68-4eb0-afb9-d908dd59d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backbone\n",
    "\n",
    "from SLRTNGT.TwoStreamNetwork.modelling.pyramid import PyramidNetwork, PyramidNetwork_v2\n",
    "\n",
    "class S3D_backbone(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        num_class=400, in_channel=3,\n",
    "        use_block=5, freeze_block=0, stride=2, pretrained_ckpt='scratch', cfg_pyramid=None):\n",
    "        super(S3D_backbone, self).__init__()\n",
    "        self.logger = get_logger()\n",
    "        self.cfg_pyramid = cfg_pyramid\n",
    "        self.backbone = S3Ds(\n",
    "            num_class=num_class,\n",
    "            in_channel=in_channel,\n",
    "            use_block=use_block, freeze_block=freeze_block, stride=stride)  \n",
    "        self.set_frozen_layers()\n",
    "        self.out_features = BLOCK2SIZE[use_block]\n",
    "        if pretrained_ckpt=='scratch':\n",
    "            self.logger.info('Train S3D backbone from scratch')\n",
    "        else:\n",
    "            #self.logger.info('Load pretrained S3D backbone from {}'.format(pretrained_ckpt))\n",
    "            self.load_s3d_model_weight(pretrained_ckpt)\n",
    "        \n",
    "        self.stage_idx = [0, 3, 6, 12]\n",
    "        self.stage_idx = self.stage_idx[:use_block]\n",
    "        self.use_block = use_block\n",
    "\n",
    "        if in_channel == 3:\n",
    "            self.branch = 'rgb'\n",
    "        else:\n",
    "            self.branch = 'pose'\n",
    "        \n",
    "        self.pyramid = None\n",
    "        self.num_levels = 3\n",
    "#        if cfg_pyramid is not None:\n",
    "#            print(\"Using Pyramid\")\n",
    "#            if branch == \"rgb\":\n",
    "#                if cfg_pyramid['version'] == 'v2':\n",
    "#                    self.num_levels = cfg_pyramid.get('num_levels', 3)\n",
    "#                    self.pyramid = PyramidNetwork_v2(channels=[832,480,192,64], kernel_size=1, num_levels=self.num_levels, temp_scale=[2,1,1], spat_scale=[2,2,2])\n",
    "#                else:\n",
    "#                    self.num_levels = cfg_pyramid.get('num_levels', 4)\n",
    "#                    self.pyramid = PyramidNetwork(channels=[832,480,192,64], kernel_size=3, num_levels=self.num_levels, temp_scale=[2,1,1], spat_scale=[2,2,2])\n",
    "    \n",
    "    def load_s3d_model_weight(self, model_path):\n",
    "        if 'actioncls' in model_path:\n",
    "            #filename = glob.glob(os.path.join(model_path, '*.pt'))\n",
    "            checkpoint = torch.load(model_path, map_location='cuda')\n",
    "            state_dict = checkpoint\n",
    "            new_dict = {}\n",
    "            for k,v in state_dict.items():\n",
    "                k = k.replace('module.', 'backbone.')\n",
    "                new_dict[k] = v\n",
    "            state_dict = new_dict\n",
    "            try: self.load_state_dict(state_dict)\n",
    "            except: neq_load_customized(self, state_dict, verbose=False)\n",
    "        elif 'glosscls' in model_path:\n",
    "            #filename = glob.glob(os.path.join(model_path, '*.pth.tar'))\n",
    "            checkpoint = torch.load(model_path, map_location='cuda')\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            try:\n",
    "                self.load_state_dict(state_dict)\n",
    "            except:\n",
    "                neq_load_customized(self, state_dict, verbose=False)      \n",
    "        else:\n",
    "            raise ValueError  \n",
    "\n",
    "    def set_train(self):\n",
    "        self.train()\n",
    "        for m in getattr(self.backbone,'frozen_modules',[]):\n",
    "            m.eval()\n",
    "\n",
    "    def get_frozen_layers(self):\n",
    "        return getattr(self.backbone,'frozen_modules',[])\n",
    "    def set_frozen_layers(self):\n",
    "        for m in getattr(self.backbone,'frozen_modules',[]):\n",
    "            for param in m.parameters():\n",
    "                param.requires_grad = False\n",
    "            m.eval()\n",
    "\n",
    "#    def alter_keypoints(self, keypoints, sgn_lengths=None):\n",
    "#        (B, T_in, H, W) = keypoints.shape\n",
    "#        C = 1\n",
    "#        keypoints = keypoints.reshape(B, C, T_in, H, W)\n",
    "#        \n",
    "#        sgn_lengths=torch.full(size=(1,B), fill_value=T_in)\n",
    "#        sgn_lengths=sgn_lengths[0]\n",
    "#        return keypoints, sgn_lengths\n",
    "\n",
    "    def forward(self, sgn_videos, sgn_lengths=None): ## sgn_lengths originally None  or torch.tensor([1])\n",
    "        if self.branch == \"rgb\":\n",
    "            (B, C, T_in, H, W) = sgn_videos.shape\n",
    "            sgn_lengths=torch.full(size=(1,B), fill_value=T_in)\n",
    "            sgn_lengths=sgn_lengths[0]\n",
    "        \n",
    "        fea_lst = []\n",
    "        for i, layer in enumerate(self.backbone.base):\n",
    "            sgn_videos = layer(sgn_videos)\n",
    "            if i in self.stage_idx[self.use_block-self.num_levels:]:\n",
    "                fea_lst.append(sgn_videos)\n",
    "\n",
    "        sgn_mask_lst, valid_len_out_lst = [], []\n",
    "\n",
    "        feat3d = fea_lst[-1]\n",
    "        B, _, T_out, _, _ = feat3d.shape\n",
    "        pooled_sgn_feature = torch.mean(feat3d, dim=[3,4]) #leaves B, D, T_out\n",
    "        #pooled_sgn_feature = torch.mean(pooled_sgn_feature, dim=[2], keepdims=True) #B, D, T_out  ###alteration\n",
    "        sgn = torch.transpose(pooled_sgn_feature, 1, 2) #b, t_OUT, d\n",
    "        sgn_mask = torch.zeros([B,1,T_out], dtype=torch.bool, device=sgn.device)\n",
    "        #print(\"T_out \" + str(T_out))\n",
    "        valid_len_out = torch.floor(sgn_lengths*T_out/T_in).long() #B,   ##torch.floor(sgn_lengths*T_out/T_in).long() #B,\n",
    "        for bi in range(B):\n",
    "            sgn_mask[bi, :, :valid_len_out[bi]] = True\n",
    "        sgn_mask_lst.append(sgn_mask)\n",
    "        valid_len_out_lst.append(valid_len_out)\n",
    "\n",
    "       # print(\"sign mask \" + str(sgn_mask.shape))\n",
    "    \n",
    "        return {'sgn_feature':fea_lst[-1], 'sgn_mask':sgn_mask_lst, \n",
    "                'valid_len_out': valid_len_out_lst, \n",
    "                'fea_lst': fea_lst, 'sgn':sgn}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7751474-8c25-4b24-8d24-e9d740a63556",
   "metadata": {},
   "source": [
    "## Head network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e219ffdf-ff0d-4d91-b662-7fca1b589fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"SLRTNGT/TwoStreamNetwork/experiments/outputs/SingleStream/head_rgb_input/test.pkl\", 'rb') as f:\n",
    "                    split_data = pickle.load(f)\n",
    "\n",
    "#print(split_data[3]['sign'].shape)\n",
    "#print(split_data[3])\n",
    "\n",
    "## So we know the extracted features are only from the S3D, since the size is (number of frames)/4 by 832, \n",
    "## the input for the head are T (num frames/4) by 843\n",
    "## so the head input size varies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b98bd-c7e1-40b2-b030-73b135342b31",
   "metadata": {},
   "source": [
    "##### VisualHead Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140ff4c1-8db8-4696-b149-589aa4cf5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, embedding_size, projection_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.net = nn.Sequential(nn.Linear(self.embedding_size, projection_hidden_size),\n",
    "                                nn.BatchNorm1d(projection_hidden_size),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(projection_hidden_size, self.embedding_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, l, c = x.shape\n",
    "        x = x.reshape(-1,c)#x.view(-1,c)\n",
    "        x = self.net(x)\n",
    "        return x.reshape(b,l,c)#x.view(b, l, c)    ##x.reshape(b,l,c)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-forward layer\n",
    "    Projects to ff_size and then back down to input_size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, ff_size, dropout=0.1, kernel_size=1,\n",
    "        skip_connection=True):\n",
    "        \"\"\"\n",
    "        Initializes position-wise feed-forward layer.\n",
    "        :param input_size: dimensionality of the input.\n",
    "        :param ff_size: dimensionality of intermediate representation\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(input_size, eps=1e-6)\n",
    "        self.kernel_size = kernel_size\n",
    "        if type(self.kernel_size)==int:\n",
    "            conv_1 = nn.Conv1d(input_size, ff_size, kernel_size=kernel_size, stride=1, padding='same')\n",
    "            conv_2 = nn.Conv1d(ff_size, input_size, kernel_size=kernel_size, stride=1, padding='same')\n",
    "            self.pwff_layer = nn.Sequential(\n",
    "                conv_1,\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                conv_2,\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "        elif type(self.kernel_size)==list:\n",
    "            pwff = []\n",
    "            first_conv = nn.Conv1d(input_size, ff_size, kernel_size=kernel_size[0], stride=1, padding='same')\n",
    "            pwff += [first_conv, nn.ReLU(), nn.Dropout(dropout)]\n",
    "            for ks in kernel_size[1:-1]:\n",
    "                conv = nn.Conv1d(ff_size, ff_size, kernel_size=ks, stride=1, padding='same')\n",
    "                pwff += [conv, nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last_conv = nn.Conv1d(ff_size, input_size, kernel_size=kernel_size[-1], stride=1, padding='same')\n",
    "            pwff += [last_conv, nn.Dropout(dropout)]\n",
    "\n",
    "            self.pwff_layer = nn.Sequential(\n",
    "                *pwff\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.skip_connection=skip_connection\n",
    "        if not skip_connection:\n",
    "            print('Turn off skip_connection in PositionwiseFeedForward')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.layer_norm(x)\n",
    "        x_t = x_norm.transpose(1,2)\n",
    "        x_t = self.pwff_layer(x_t)\n",
    "        if self.skip_connection:\n",
    "            return x_t.transpose(1,2)+x\n",
    "        else:\n",
    "            return x_t.transpose(1,2)\n",
    "\n",
    "class MaskedNorm(nn.Module):\n",
    "    \"\"\"\n",
    "        Original Code from:\n",
    "        https://discuss.pytorch.org/t/batchnorm-for-different-sized-samples-in-batch/44251/8\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features=512, norm_type='sync_batch', num_groups=1):\n",
    "        super().__init__()\n",
    "        self.norm_type = norm_type\n",
    "        if self.norm_type == \"batch\":\n",
    "            raise ValueError(\"Please use sync_batch\")\n",
    "            self.norm = nn.BatchNorm1d(num_features=num_features)\n",
    "        elif self.norm_type == 'sync_batch':\n",
    "            self.norm = nn.SyncBatchNorm(num_features=num_features)\n",
    "        elif self.norm_type == \"group\":\n",
    "            self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=num_features)\n",
    "        elif self.norm_type == \"layer\":\n",
    "            self.norm = nn.LayerNorm(normalized_shape=num_features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported Normalization Layer\")\n",
    "\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor):\n",
    "        if self.training:\n",
    "            reshaped = x.reshape([-1, self.num_features])\n",
    "            reshaped_mask = mask.reshape([-1, 1]) > 0\n",
    "            selected = torch.masked_select(reshaped, reshaped_mask).reshape(\n",
    "                [-1, self.num_features]\n",
    "            )\n",
    "            batch_normed = self.norm(selected)\n",
    "            scattered = reshaped.masked_scatter(reshaped_mask, batch_normed)\n",
    "            return scattered.reshape([x.shape[0], -1, self.num_features])\n",
    "        else:\n",
    "            reshaped = x.reshape([-1, self.num_features])\n",
    "            batched_normed = self.norm(reshaped)\n",
    "            return batched_normed.reshape([x.shape[0], -1, self.num_features])\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-compute position encodings (PE).\n",
    "    In forward pass, this adds the position-encodings to the\n",
    "    input for as many time steps as necessary.\n",
    "    Implementation based on OpenNMT-py.\n",
    "    https://github.com/OpenNMT/OpenNMT-py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int = 0, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Positional Encoding with maximum length max_len\n",
    "        :param size:\n",
    "        :param max_len:\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        if size % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"Cannot use sin/cos positional encoding with \"\n",
    "                \"odd dim (got dim={:d})\".format(size)\n",
    "            )\n",
    "        pe = torch.zeros(max_len, size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, size, 2, dtype=torch.float) * -(math.log(10000.0) / size))\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: [1, size, max_len]\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dim = size\n",
    "\n",
    "    def forward(self, emb):\n",
    "        \"\"\"Embed inputs.\n",
    "        Args:\n",
    "            emb (FloatTensor): Sequence of word vectors\n",
    "                ``(seq_len, batch_size, self.dim)``\n",
    "        \"\"\"\n",
    "        # Add position encodings\n",
    "        return emb + self.pe[:, : emb.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55d2bc-7640-40e1-b74c-b2f1220d38af",
   "metadata": {},
   "source": [
    "##### VisualHead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15bda4a-f043-4d40-95fc-2bc0ed4e9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualHead(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        cls_num, input_size=832, hidden_size=512, ff_size=2048, pe=True,\n",
    "        ff_kernelsize=3, pretrained_ckpt=None, is_empty=False, frozen=False, \n",
    "        plus_conv_cfg={},\n",
    "        ssl_projection_cfg={}):\n",
    "        super().__init__()\n",
    "        self.is_empty = is_empty\n",
    "        self.plus_conv_cfg = plus_conv_cfg\n",
    "        self.ssl_projection_cfg = ssl_projection_cfg\n",
    "\n",
    "        if input_size == 832:\n",
    "            self.branch = \"s3d\"\n",
    "        else:\n",
    "            self.branch = \"pose\"\n",
    "            input_size = 33*3\n",
    "\n",
    "        if is_empty==False:\n",
    "            self.frozen = frozen\n",
    "            self.hidden_size = hidden_size\n",
    "\n",
    "            if input_size is None:\n",
    "                self.fc1 = nn.Identity()\n",
    "            else:\n",
    "                self.fc1 = torch.nn.Linear(input_size, self.hidden_size)\n",
    "            self.bn1 = MaskedNorm(num_features=self.hidden_size, norm_type='sync_batch')\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.dropout1 = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "            if pe:\n",
    "                self.pe = PositionalEncoding(self.hidden_size)\n",
    "            else:\n",
    "                self.pe = torch.nn.Identity()\n",
    "\n",
    "            self.feedforward = PositionwiseFeedForward(input_size=self.hidden_size,\n",
    "                ff_size=ff_size,\n",
    "                dropout=0.1, kernel_size=ff_kernelsize, skip_connection=True)\n",
    "            \n",
    "            self.layer_norm = torch.nn.LayerNorm(self.hidden_size, eps=1e-6)\n",
    "\n",
    "            if plus_conv_cfg!={}:\n",
    "                plus_convs = []\n",
    "                for i in range(plus_conv_cfg['num_layer']):\n",
    "                    plus_convs.append(nn.Conv1d(self.hidden_size, self.hidden_size, \n",
    "                        kernel_size=plus_conv_cfg['kernel_size'], stride=plus_conv_cfg['stride'], padding_mode='replicate'))\n",
    "                self.plus_conv = nn.Sequential(*plus_convs)\n",
    "            else:\n",
    "                self.plus_conv = nn.Identity()\n",
    "\n",
    "            if ssl_projection_cfg!={}:\n",
    "                self.ssl_projection = MLPHead(embedding_size=self.hidden_size, \n",
    "                    projection_hidden_size=ssl_projection_cfg['hidden_size'])\n",
    "\n",
    "            self.gloss_output_layer = torch.nn.Linear(self.hidden_size, cls_num)\n",
    "\n",
    "            if self.frozen:\n",
    "                self.frozen_layers = [self.fc1, self.bn1, self.relu1,  self.pe, self.dropout1, self.feedforward, self.layer_norm]\n",
    "                for layer in self.frozen_layers:\n",
    "                    for name, param in layer.named_parameters():\n",
    "                        param.requires_grad = False\n",
    "                    layer.eval()\n",
    "        else:\n",
    "            self.gloss_output_layer = torch.nn.Linear(input_size, cls_num)\n",
    "        if pretrained_ckpt:\n",
    "            self.load_from_pretrained_ckpt(pretrained_ckpt)\n",
    "\n",
    "    def load_from_pretrained_ckpt(self, pretrained_ckpt):\n",
    "        logger = Logger     # get_logger()\n",
    "        checkpoint = torch.load(pretrained_ckpt, map_location='cuda')#['model_state']\n",
    "        load_dict = {}\n",
    "        for k,v in checkpoint.items():\n",
    "            if 'recognition_network.visual_head.' in k:\n",
    "                load_dict[k.replace('recognition_network.visual_head.','')] = v\n",
    "        self.load_state_dict(load_dict)\n",
    "        logger.info('Load Visual Head from pretrained ckpt {}'.format(pretrained_ckpt))\n",
    "\n",
    "    def alter_keypoints(self, keypoints):\n",
    "        (B, T_in, H, W) = keypoints.shape\n",
    "        keypoints = keypoints.reshape(B, T_in, H*W)\n",
    "        return keypoints\n",
    "    \n",
    "    def forward(self, x, mask=None, valid_len_in=None):\n",
    "        if self.branch == \"pose\":\n",
    "            x = self.alter_keypoints(x)\n",
    "            B, Tin, D = x.shape \n",
    "            valid_len_in=torch.full(size=(1,B), fill_value=Tin)\n",
    "            valid_len_in=valid_len_in[0]\n",
    "        else:\n",
    "            B, Tin, D = x.shape \n",
    "        Tout = 1  ## ALTERATION\n",
    "        if self.is_empty==False:\n",
    "            if not self.frozen:\n",
    "                #projection 1\n",
    "                x = self.fc1(x)\n",
    "                if self.branch==\"s3d\":\n",
    "                    x = self.bn1(x, mask)\n",
    "                x = self.relu1(x)\n",
    "                #pe\n",
    "                x = self.pe(x)\n",
    "                x = self.dropout1(x)\n",
    "\n",
    "                #feedforward\n",
    "                x = self.feedforward(x)\n",
    "                x = self.layer_norm(x)\n",
    "\n",
    "                x = x.transpose(1,2)\n",
    "                x = self.plus_conv(x)\n",
    "                x = x.transpose(1,2)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    for ii, layer in enumerate(self.frozen_layers):\n",
    "                        layer.eval()\n",
    "                        if ii==1:\n",
    "                            if self.branch == \"s3d\":\n",
    "                                x = layer(x, mask)\n",
    "                        else:\n",
    "                            x = layer(x)\n",
    "                x = x.transpose(1,2)\n",
    "                x = self.plus_conv(x)\n",
    "                x = x.transpose(1,2)\n",
    "            #print(\"here!\")\n",
    "            #print(x.shape)\n",
    "            #if branch == \"pose\":\n",
    "            #    x = torch.mean(x, dim=1, keepdim=True) ###\n",
    "\n",
    "        #classification\n",
    "        logits = self.gloss_output_layer(x) #B,T,V\n",
    "\n",
    "        #softmax\n",
    "        gloss_probabilities_log = logits.log_softmax(2) \n",
    "        gloss_probabilities = logits.softmax(2)\n",
    "\n",
    "        if self.plus_conv_cfg!={}:\n",
    "            B, Tout, D = x.shape\n",
    "            valid_len_out = torch.floor(valid_len_in*Tout/Tin).long() #B,\n",
    "        else:\n",
    "            valid_len_out = valid_len_in\n",
    "        if self.ssl_projection_cfg!={}:\n",
    "            x_ssl = self.ssl_projection(x)\n",
    "            if self.ssl_projection_cfg['normalize']==True:\n",
    "                x_ssl = F.normalize(x_ssl, dim=-1)\n",
    "        else:\n",
    "            x_ssl = None\n",
    "\n",
    "        ## These are all the different features we can use\n",
    "        return {'gloss_feature_ssl':x_ssl, \n",
    "                'gloss_feature': x,\n",
    "                'gloss_feature_norm': F.normalize(x, dim=-1),\n",
    "                'gloss_logits':logits, \n",
    "                'gloss_probabilities_log':gloss_probabilities_log,\n",
    "                'gloss_probabilities': gloss_probabilities,\n",
    "                'valid_len_out':valid_len_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce55d3-2915-41b9-93c4-33096b20ca4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f4b6fe-02a6-4bf9-929e-e1073516ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2000\n",
      "torch.Size([1, 8, 33, 3])\n",
      "\n",
      "\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 2000])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\novan\\AppData\\Local\\Temp\\ipykernel_15536\\577407715.py:99: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  keypoints = torch.tensor(snippet)\n"
     ]
    }
   ],
   "source": [
    "## We test the pretrained S3D model on a video from the CorpusNGT dataset, and it indeed works.\n",
    "\n",
    "def transform(snippet):\n",
    "    ''' stack & noralization '''\n",
    "    snippet = np.concatenate(snippet, axis=-1)\n",
    "    snippet = torch.from_numpy(snippet).permute(2, 0, 1).contiguous().float()\n",
    "    snippet = snippet.mul_(2.).sub_(255).div(255)\n",
    "    snippet = snippet.view(1,-1,3,snippet.size(1),snippet.size(2)).permute(0,2,1,3,4) \n",
    "    print(snippet.shape)\n",
    "    return snippet\n",
    "    # returns tensor in size [batch, channels, frames, height, width]\n",
    "    # all values normalized\n",
    "\n",
    "def main():\n",
    "    ''' Output the top 5 Kinetics classes predicted by the model or the gloss features'''\n",
    "    \n",
    "    #path_sample = './sample'\n",
    "    path_sample = 'Data/CorpusNGT/gloss_split/2/28'\n",
    "    #keypoint_test = np.load(\"Data/CorpusNGT/gloss_split/0/0/frame0_array.npy\")\n",
    "    #print(keypoint_test.shape)\n",
    "    #print(keypoint_test)\n",
    "    \n",
    "    file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/s3ds_glosscls_ckpt/epoch299.pth.tar'\n",
    "    #file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/csl-daily_s2g/ckpts/best.ckpt'\n",
    "    #file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/s3ds_actioncls_ckpt/S3D_kinetics400.pt'\n",
    "    \n",
    "    class_names = pd.read_pickle(\"SLRTNGT/TwoStreamNetwork/data/csl-daily/gloss2ids.pkl\")\n",
    "    class_names = {K-4:V for (V,K) in [x for x in class_names.items()][4:]}\n",
    "    #class_names = pd.read_csv(\"Data/Kinetics_labels/kinetics_400_labels.csv\")\n",
    "    \n",
    "    num_class = len(class_names)\n",
    "    print(\"Number of classes: \" + str(num_class))\n",
    "    #num_class = 400\n",
    "\n",
    "    #state = \"features\"\n",
    "    state = \"kinetics\"\n",
    "\n",
    "    #S3Dtype = \"S3D\"\n",
    "    S3Dtype = \"Backbone\"\n",
    "\n",
    "    if S3Dtype == \"S3D\":\n",
    "        ### Perform S3D feature extraction\n",
    "        model = S3Ds(num_class, use_block=4, freeze_block=1)  ## 4 and 1 for gloss\n",
    "    \n",
    "        # load the weight file and copy the parameters\n",
    "        if os.path.isfile(file_weight):\n",
    "            print ('loading weight file')\n",
    "            weight_dict = torch.load(file_weight)\n",
    "            model_dict = model.state_dict()\n",
    "            for name, param in weight_dict.items(): # name is the name of the module, param is the weights\n",
    "                if 'module' in name:\n",
    "                    name = '.'.join(name.split('.')[1:])\n",
    "                if name in model_dict:\n",
    "                    if param.size() == model_dict[name].size():\n",
    "                        model_dict[name].copy_(param)\n",
    "                    else:\n",
    "                        print (' size? ' + name, param.size(), model_dict[name].size())\n",
    "                else:\n",
    "                    print (' name? ' + name)\n",
    "    \n",
    "            print (' loaded')\n",
    "        else:\n",
    "            print ('weight file?')\n",
    "\n",
    "        model = model.cuda()\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    else:\n",
    "        #cfgpyramid = {\"version\":\"v2\", \"num_levels\":3}\n",
    "        #print(cfgpyramid)\n",
    "        if state == \"features\":\n",
    "            model = S3D_backbone(num_class, use_block=4, freeze_block=1, pretrained_ckpt=file_weight, cfg_pyramid=None)  ## 4 and 1 for gloss\n",
    "            model = model.cuda()\n",
    "        else:\n",
    "            pass\n",
    "            #model = S3D_backbone(num_class, in_channel=1, use_block=4, freeze_block=1, pretrained_ckpt=file_weight, cfg_pyramid=None)\n",
    "    \n",
    "    #model.eval()\n",
    "\n",
    "    if state == \"features\":\n",
    "        # read all the frames of sample clip\n",
    "        list_frames = [f for f in os.listdir(path_sample) if os.path.isfile(os.path.join(path_sample, f)) and f.split(\".\")[1] == \"jpg\"]\n",
    "        list_frames.sort()\n",
    "        snippet = []\n",
    "        for frame in list_frames:\n",
    "            img = cv2.imread(os.path.join(path_sample, frame))\n",
    "            img = cv2.resize(img, [270, 270])\n",
    "            img = img[...,::-1]\n",
    "            snippet.append(img)\n",
    "            #snippet.append(img) ## added because not enough frames in test\n",
    "        clip = transform(snippet)\n",
    "    else:\n",
    "        list_frames = [f for f in os.listdir(path_sample) if os.path.isfile(os.path.join(path_sample, f)) and f.split(\".\")[1] == \"npy\"]\n",
    "        list_frames.sort()\n",
    "        snippet = []\n",
    "        for frame in list_frames:\n",
    "            keypoint = np.load(os.path.join(path_sample, frame))\n",
    "            snippet.append(keypoint)\n",
    "        keypoints = torch.tensor(snippet)\n",
    "        T, H, W = keypoints.shape\n",
    "        keypoints = keypoints.reshape(1, T, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if state == \"features\":\n",
    "            if S3Dtype == \"Backbone\":\n",
    "                logits = model(clip.cuda())#.cpu()#.data#[0]\n",
    "            else:\n",
    "                logits = model(clip.cuda()).cpu()#.data#[0]\n",
    "        else:\n",
    "            logits = keypoints\n",
    "\n",
    "    if state == \"features\":\n",
    "        print ('\\nThe features outputted by pretrained S3D')\n",
    "        if S3Dtype == \"Backbone\":\n",
    "            print(logits['sgn_feature'].shape)\n",
    "            print(logits['sgn'].shape)\n",
    "            print(logits['valid_len_out'])\n",
    "        \n",
    "        else:\n",
    "            print(logits.shape)\n",
    "            print(logits)\n",
    "\n",
    "    #if state == \"kinetics\":\n",
    "    #    preds = torch.softmax(logits, 0).numpy()\n",
    "    #    sorted_indices = np.argsort(preds)[::-1][:5]\n",
    "    #    print(sorted_indices)\n",
    "    #    print(logits.shape)\n",
    "    #    print ('\\nTop 5 kinetics classes ... with probability')\n",
    "    #    for idx in sorted_indices:\n",
    "            #print(class_names['name'][idx], '...', preds[idx])\n",
    "    #        print(class_names[idx], '...', preds[idx])\n",
    "\n",
    "\n",
    "    ### Perform Visual Head feature extraction\n",
    "    #file_weight_vh = 'SLRTNGT/TwoStreamNetwork/pretrained_models/csl-daily_s2g/ckpts/best.ckpt'\n",
    "    model = VisualHead(cls_num=num_class, input_size=33*3, pe=True)  ## 4 and 1 for gloss    #pretrained_ckpt=file_weight_vh\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #print(split_data[3]['sign'])\n",
    "        if S3Dtype == \"Backbone\":\n",
    "            if state == \"features\":\n",
    "                features = model(x=logits['sgn'].cpu(), mask=logits['sgn_mask'][-1].cpu(), \n",
    "                                 #valid_len_in=logits['valid_len_out'][-1].cpu()) \n",
    "                                 valid_len_in=torch.tensor([1])) #logits['valid_len_out'][-1].cpu())  \n",
    "            else:\n",
    "                print(logits.shape)\n",
    "                logits = logits.type(torch.float32)\n",
    "                features = model(x=logits.cpu(), mask=None, \n",
    "                                 #valid_len_in=logits['valid_len_out'][-1].cpu()) \n",
    "                                 valid_len_in=torch.tensor([1])) #logits['valid_len_out'][-1].cpu())  \n",
    "        else:\n",
    "            features = model(x=logits.unsqueeze(0), mask=torch.zeros(1))  \n",
    "        print(\"\\n\")\n",
    "        print(features['gloss_feature'].shape)\n",
    "        print(features['gloss_probabilities'].shape)\n",
    "        print(features['valid_len_out'].shape)\n",
    "        #print(features['gloss_probabilities'].squeeze().shape)\n",
    "        #print(features)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57da89-73f6-4b0a-9fec-040cad7dbaf6",
   "metadata": {},
   "source": [
    "## Classification S2G Test\n",
    "\n",
    "So the pretrained S3D and visual head work can be loaded, so we can use these to extract the features for all the CorpusNGT data\n",
    "We can choose to then use either the visual features exported by the S3D, the gloss representations outputted by the VisualHead, or the gloss logits or gloss probabilities which have been classified and then softmaxed. Either way, this is the S3D is the 'cold' part of the model; we don't further train these weights. The VisualHead plus Classifier do not use pretrained weights, so if we are training for S2G this is the 'hot' part that would be trained. \n",
    "\n",
    "Next steps:\n",
    "- Test to see if a the VisualHead and Classifier trained on CorpusNGT acchieve good performance\n",
    "- See if this performance gets even better when using Mediapipe data alongside the S3D features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf1b03-1e9d-49bf-91e9-4b886467ddef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283e28ca-4663-428a-91a5-2c5e2109d564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cb649783d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import cv2 as cv\n",
    "import re\n",
    "import math\n",
    "from subprocess import check_call, PIPE, Popen\n",
    "import shlex\n",
    "import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "np.random.seed(2024)\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef40c6-2a40-4ddb-be81-105b843fceba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f8cfc4-cf00-45f7-a12a-4f2ab7acf4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2328\n"
     ]
    }
   ],
   "source": [
    "path2data = \"Data/CorpusNGT/gloss_split/\"\n",
    "\n",
    "listOfCategories = os.listdir(path2data)\n",
    "listOfCategories, len(listOfCategories)\n",
    "\n",
    "unique_glosses = np.load('Data/CorpusNGT/gloss_labels.npy',allow_pickle='TRUE').item()\n",
    "#gloss_counter = np.load('Data/CorpusNGT/gloss_counts.npy',allow_pickle='TRUE').item()\n",
    "print(len(unique_glosses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e66862-51d6-4df0-972e-cbd00bb7cd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50074, 50074, 146)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vids(path2data):\n",
    "    listOfCats = os.listdir(path2data)\n",
    "    ids = []\n",
    "    labels = []\n",
    "    usableListOfCats = []\n",
    "    for catg in listOfCats:\n",
    "        path2catg = os.path.join(path2data, catg)\n",
    "        #counter = 0\n",
    "        if len(os.listdir(path2catg)) >= 100:  # We can only use glosses which have more than 4 occurences, meaning more than 2*2 files in dir, since video and frames are in there.\n",
    "            #counter += 1\n",
    "            listOfSubCats = os.listdir(path2catg)\n",
    "            path2subCats = [os.path.join(path2catg, los) for los in listOfSubCats if os.path.isdir(os.path.join(path2catg, los))]\n",
    "\n",
    "            path2subCats = [path for path in path2subCats if len(os.listdir(path)) > 16]  \n",
    "            #14 because mediapipe data is 8, and frames another 8. And we want at least 8 frames (thats the minimum for the S3D)\n",
    "            \n",
    "            imagecount = [len(glob.glob(path+'\\*.jpg')) for path in path2subCats]\n",
    "            keypointcount = [len(glob.glob(path+'\\*.npy')) for path in path2subCats]\n",
    "            path2subCats = [path for i, path in enumerate(path2subCats) if imagecount[i]==keypointcount[i]]  \n",
    "            #We only want the files where keypoint extraction was successful, so same amount of frames as keypoint files\n",
    "\n",
    "            if len(path2subCats) >= 100: ## check that there's still at least 10 paths after removing outliers, enough for a test and training split with 5 each.\n",
    "                ids.extend(path2subCats)\n",
    "                labels.extend([catg]*len(path2subCats))\n",
    "                usableListOfCats.append(catg)\n",
    "    return ids, labels, usableListOfCats\n",
    "\n",
    "all_vids, all_labels, catgs = get_vids(path2data)\n",
    "len(all_vids), len(all_labels), len(catgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8eac27-1822-49e4-b539-4be611c30a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50074, 50074)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "ind = 0\n",
    "for uc in catgs:\n",
    "    labels_dict[uc] = ind\n",
    "    ind+=1\n",
    "\n",
    "num_classes = len(catgs)\n",
    "unique_ids = [id_ for id_, label in zip(all_vids,all_labels) if labels_dict[label]<num_classes]\n",
    "unique_labels = [label for id_, label in zip(all_vids,all_labels) if labels_dict[label]<num_classes]\n",
    "len(unique_ids), len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bc77e4-80e7-49f8-a6ee-5f7c6a9aa1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45066 45066\n",
      "5008 5008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0)\n",
    "train_indx, test_indx = next(sss.split(unique_ids, unique_labels))\n",
    "\n",
    "train_ids = [unique_ids[ind] for ind in train_indx]\n",
    "train_labels = [unique_labels[ind] for ind in train_indx]\n",
    "print(len(train_ids), len(train_labels))\n",
    "\n",
    "test_ids = [unique_ids[ind] for ind in test_indx]\n",
    "test_labels = [unique_labels[ind] for ind in test_indx]\n",
    "print(len(test_ids), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c01ddfe-ad68-4bd5-842b-529e77863f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 8  # nr of frames we use\n",
    "#data_type = \"image\"\n",
    "#data_type = \"keypoint\"\n",
    "data_type = \"both\"\n",
    "if data_type == \"image\":\n",
    "    hv, wv = 270, 270\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "elif data_type == \"keypoint\":\n",
    "    hk, wk = 33, 3\n",
    "else:\n",
    "    hv, wv = 270, 270\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    hk, wk = 33, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad661f38-3fe1-495e-9bd7-f7fcde627999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, ids, labels, transform1, transform2):\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        path2imgs = glob.glob(self.ids[idx]+\"/*.jpg\")\n",
    "        path2imgs = path2imgs[:timesteps]\n",
    "        label = labels_dict[self.labels[idx]]\n",
    "        frames = []\n",
    "        for p2i in path2imgs:\n",
    "            frame = Image.open(p2i)\n",
    "            frames.append(frame)\n",
    "\n",
    "        seed = np.random.randint(1e9)\n",
    "        transform_frames = np.random.randint(low=0, high=2) # 50 50 chance of doing the transforms\n",
    "        frames_tr = []\n",
    "        for frame in frames:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            if transform_frames >=1:\n",
    "                frame = self.transform1(frame)\n",
    "            else:\n",
    "                frame = self.transform2(frame)\n",
    "            frames_tr.append(frame)\n",
    "        if len(frames_tr)>0:\n",
    "            frames_tr = torch.stack(frames_tr)\n",
    "        return frames_tr, label\n",
    "\n",
    "class KeyPointDataset(Dataset):\n",
    "    def __init__(self, ids, labels, transform):\n",
    "        self.transform = transform\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        path2keypoints=glob.glob(self.ids[idx]+\"/*.npy\")\n",
    "        path2keypoints = path2keypoints[:timesteps]\n",
    "        label = labels_dict[self.labels[idx]]\n",
    "        frames = []\n",
    "        for p2i in path2keypoints:\n",
    "            frame = np.load(p2i)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        seed = np.random.randint(1e9)\n",
    "        frames_tr = []\n",
    "        for frame in frames:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            frames_tr.append(torch.from_numpy(frame))\n",
    "        if len(frames_tr)>0:\n",
    "            frames_tr = torch.stack(frames_tr)\n",
    "\n",
    "        frames_tr = frames_tr.type(torch.float32)\n",
    "        \n",
    "        return frames_tr, label\n",
    "\n",
    "# define helper functions\n",
    "\n",
    "#def transform_frames(frames, model_type=\"rnn\"):\n",
    "#    h, w = 270, 270\n",
    "#    mean = [0.485, 0.456, 0.406]\n",
    "#    std = [0.229, 0.224, 0.225]\n",
    "#    test_transformer = transforms.Compose([\n",
    "#                transforms.Resize((h,w)),\n",
    "#                transforms.ToTensor(),\n",
    "#                transforms.Normalize(mean, std)])\n",
    "#\n",
    "#    frames_tr = []\n",
    "#    for frame in frames:\n",
    "#        frame = Image.fromarray(frame)\n",
    "#        frame_tr = test_transformer(frame)\n",
    "#        frames_tr.append(frame_tr)\n",
    "#    imgs_tensor = torch.stack(frames_tr)\n",
    "#\n",
    "#    if model_type != \"rnn\":\n",
    "#        imgs_tensor = torch.transpose(imgs_tensor, 1, 0)\n",
    "#    imgs_tensor = imgs_tensor.unsqueeze(0)\n",
    "#    return imgs_tensor\n",
    "\n",
    "def denormalize(x_, mean, std):\n",
    "    x = x_.clone()\n",
    "    for i in range(3):\n",
    "        x[i] = x[i]*std[i]+mean[i]\n",
    "    x = to_pil_image(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e809d576-a79b-459e-b375-90a72710beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d498b98-d82e-45d5-a4d5-6582ddded2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45066\n",
      "45066\n",
      "5008\n",
      "5008\n"
     ]
    }
   ],
   "source": [
    "train_transformer_augmentation1 = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomAffine(degrees=5, translate=(0.15,0.15)),\n",
    "            v2.ColorJitter(brightness=.2, hue=.15),\n",
    "            transforms.Resize((hv,wv)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            ])\n",
    "train_transformer_augmentation2 = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0),\n",
    "            transforms.RandomAffine(degrees=2, translate=(0.05,0.05)),\n",
    "            transforms.Resize((hv,wv)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            ])\n",
    "\n",
    "train_transformer_keypoints = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "doubled_train_ids = [id for id in train_ids for _ in (0, 1)]\n",
    "tripled_train_ids = [id for id in doubled_train_ids for _ in (0, 1)]\n",
    "doubled_train_labels = [id for id in train_labels for _ in (0, 1)]\n",
    "tripled_train_labels = [id for id in doubled_train_labels for _ in (0, 1)]\n",
    "\n",
    "if data_type == \"image\": \n",
    "    train_ds = VideoDataset(ids= tripled_train_ids, labels= tripled_train_labels,  #double the dataset\n",
    "                            transform1= train_transformer_augmentation1, transform2= train_transformer_augmentation2)\n",
    "    print(len(train_ds))\n",
    "elif data_type == \"keypoint\": \n",
    "    train_ds = KeyPointDataset(ids= train_ids, labels= train_labels, transform= train_transformer_keypoints)\n",
    "    print(len(train_ds))\n",
    "else:\n",
    "    train_ds_video = VideoDataset(ids= train_ids, labels= train_labels, transform1= train_transformer_augmentation2, transform2= train_transformer_augmentation2)\n",
    "    train_ds_keypoints = KeyPointDataset(ids= train_ids, labels= train_labels, transform= train_transformer_keypoints)\n",
    "    print(len(train_ds_video))\n",
    "    print(len(train_ds_video))\n",
    "\n",
    "test_transformer = transforms.Compose([\n",
    "            transforms.Resize((hv,wv)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            ])\n",
    "\n",
    "test_transformer_keypoints = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "if data_type == \"image\":\n",
    "    test_ds = VideoDataset(ids= test_ids, labels= test_labels, transform1= test_transformer, transform2= test_transformer)\n",
    "    print(len(test_ds))\n",
    "elif data_type == \"keypoint\":\n",
    "    test_ds = KeyPointDataset(ids= test_ids, labels= test_labels, transform= train_transformer_keypoints)\n",
    "    print(len(test_ds))\n",
    "else:\n",
    "    test_ds_video = VideoDataset(ids= test_ids, labels= test_labels, transform1= test_transformer, transform2= test_transformer)\n",
    "    test_ds_keypoints = KeyPointDataset(ids= test_ids, labels= test_labels, transform= train_transformer_keypoints)\n",
    "    print(len(test_ds_video))\n",
    "    print(len(test_ds_keypoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f147eb38-1763-4cc9-839e-d361d29572cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_video(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    imgs_tensor = torch.transpose(imgs_tensor, 2, 1) ##\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor\n",
    "\n",
    "def collate_fn_keypoint(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor\n",
    "\n",
    "batch_size = 3\n",
    "train_dl_video = DataLoader(train_ds_video, batch_size= batch_size,\n",
    "                      shuffle=True, collate_fn= collate_fn_video)\n",
    "test_dl_video = DataLoader(test_ds_video, batch_size= batch_size,\n",
    "                     shuffle=False, collate_fn= collate_fn_video)\n",
    "train_dl_keypoints = DataLoader(train_ds_keypoints, batch_size= batch_size,\n",
    "                      shuffle=True, collate_fn= collate_fn_keypoint)\n",
    "test_dl_keypoints = DataLoader(test_ds_keypoints, batch_size= batch_size,\n",
    "                     shuffle=False, collate_fn= collate_fn_keypoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0e4a7-0998-449a-9dcb-4aff48aa3cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e82a609-9975-4fa7-9cb9-60315a8ff2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 33, 3]), 99, tensor(-1.1797), tensor(1.4760))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "#imgs, label = test_ds_video[13]\n",
    "imgs, label = test_ds_keypoints[13]\n",
    "imgs.shape, label, torch.min(imgs), torch.max(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b4b3aea-9725-4aac-b08d-e3056822a93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "## For image condition, not keypoints\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for ii,img in enumerate(imgs[::2]):\n",
    "    plt.subplot(2,2,ii+1)\n",
    "    plt.imshow(denormalize(img, mean, std))\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d88abf-f597-42ed-b56a-2e27ff85ad21",
   "metadata": {},
   "source": [
    "#### Define model parameters and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8237e0a-d536-4096-a011-1363314104d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 146\n",
      "loading weight file\n",
      " name? epoch\n",
      " name? state_dict\n",
      " name? best_acc\n",
      " name? optimizer\n",
      " loaded\n"
     ]
    }
   ],
   "source": [
    "class S3D_VisualHead(nn.Module):\n",
    "    def __init__(self, S3D, VisualHeadV, VisualHeadK):\n",
    "        super(S3D_VisualHead, self).__init__()\n",
    "        self.hidden_size = 512\n",
    "        self.S3D = S3D\n",
    "        self.VisualHeadV = VisualHeadV\n",
    "        self.VisualHeadK = VisualHeadK\n",
    "\n",
    "        # Pyramid\n",
    "        self.fc1 = nn.Linear(num_class, self.hidden_size)\n",
    "        #self.bn1 = MaskedNorm(num_features=self.hidden_size, norm_type='sync_batch')\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.conv = nn.Conv1d(2, 1, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        \n",
    "        self.final_classifier = nn.Linear(self.hidden_size, num_class)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  ## check dim\n",
    "        #self.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "    def forward(self, x_v, x_k):\n",
    "        s3d_outputs = self.S3D(x_v)\n",
    "        vh_outputs_v = self.VisualHeadV(s3d_outputs['sgn'].cpu(), mask=s3d_outputs['sgn_mask'][-1].cpu(), \n",
    "                                        valid_len_in=s3d_outputs['valid_len_out'][-1].cpu())\n",
    "                                        #valid_len_in=torch.ones([batch_size]))\n",
    "                                        #valid_len_in=torch.ones(size=[batch_size], dtype=torch.int64))\n",
    "        vh_outputs_k = self.VisualHeadK(x_k.cpu(), mask=None, \n",
    "                                        #valid_len_in=torch.tensor([1]).cpu())\n",
    "                                        valid_len_in=s3d_outputs['valid_len_out'][-1].cpu())\n",
    "                                        \n",
    "        #print(vh_outputs_v['gloss_logits'].shape)\n",
    "        #print(vh_outputs_k['gloss_logits'].shape)\n",
    "        x = torch.cat((vh_outputs_v['gloss_probabilities'], vh_outputs_k['gloss_probabilities']), dim=1)  ## Check dim and pick what output\n",
    "        x_v = vh_outputs_v['gloss_probabilities']\n",
    "        x_k = vh_outputs_k['gloss_probabilities']\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.final_classifier(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x, x_v, x_k, vh_outputs_v[\"valid_len_out\"], vh_outputs_k[\"valid_len_out\"]\n",
    "\n",
    "#file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/csl-daily_s2t/ckpts/best.ckpt'\n",
    "#file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/s3ds_actioncls_ckpt/S3D_kinetics400.pt'\n",
    "file_weight = 'SLRTNGT/TwoStreamNetwork/pretrained_models/s3ds_glosscls_ckpt/epoch299.pth.tar'\n",
    "\n",
    "class_names = catgs\n",
    "\n",
    "num_class = len(class_names)\n",
    "print(\"Number of classes: \" + str(num_class))\n",
    "\n",
    "# Create S3D and load state_dicts    \n",
    "#S3D = S3D(num_class, use_block=4, freeze_block=1)  ## 4 and 1 for gloss\n",
    "S3D = S3D_backbone(num_class, in_channel=3, use_block=4, freeze_block=1, pretrained_ckpt=file_weight, cfg_pyramid=None)  ## 4 and 1 for gloss\n",
    "S3D = S3D.cuda()\n",
    "# load the weight file of S3D and copy the parameters\n",
    "if os.path.isfile(file_weight):\n",
    "    print ('loading weight file')\n",
    "    weight_dict = torch.load(file_weight)\n",
    "    model_dict = S3D.state_dict()\n",
    "    for name, param in weight_dict.items(): # name is the name of the module, param is the weights\n",
    "        if 'module' in name:\n",
    "            name = '.'.join(name.split('.')[1:])\n",
    "        if name in model_dict:\n",
    "            if param.size() == model_dict[name].size():\n",
    "                model_dict[name].copy_(param)\n",
    "            else:\n",
    "                print (' size? ' + name, param.size(), model_dict[name].size())\n",
    "        else:\n",
    "            print (' name? ' + name)\n",
    "    print (' loaded')\n",
    "else:\n",
    "    print ('weight file?')\n",
    "S3Dmodel = S3D.cuda()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "## Create VisualHeads\n",
    "VisualHeadV = VisualHead(num_class, pe=True).cpu()\n",
    "VisualHeadK = VisualHead(num_class, input_size=33*3, pe=True).cpu()\n",
    "\n",
    "model = S3D_VisualHead(S3Dmodel, VisualHeadV, VisualHeadK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb47e48-c209-4d0e-8e34-cd0a9698da20",
   "metadata": {},
   "source": [
    "##### Calculate Loss etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6850e567-4e32-40de-ae53-b536074bd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import tqdm\n",
    "\n",
    "def plot_loss(loss_hist, metric_hist):\n",
    "\n",
    "    num_epochs= len(loss_hist[\"train\"])\n",
    "\n",
    "    plt.title(\"Train-|Val Loss\")\n",
    "    plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
    "    plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Train-Val Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1), metric_hist[\"train\"],label=\"train\")\n",
    "    plt.plot(range(1,num_epochs+1), metric_hist[\"val\"],label=\"val\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_recognition_loss(loss_func, gloss_labels, gloss_lengths, gloss_predictions, input_lengths):\n",
    "\n",
    "    loss = loss_func(\n",
    "        log_probs = gloss_predictions.permute(1,0,2), #T,N,C\n",
    "        targets = gloss_labels,\n",
    "        input_lengths = input_lengths,\n",
    "        target_lengths = gloss_lengths\n",
    "    )\n",
    "    loss = loss/gloss_predictions.shape[0]\n",
    "    return loss\n",
    "\n",
    "def metrics_batch(output, target):\n",
    "    batch_correct = 0\n",
    "    for i, batch_item in enumerate(output):\n",
    "        pred = batch_item.argmax(keepdim=False)\n",
    "        corrects = 0\n",
    "        for item in target:\n",
    "            corrects += batch_item.eq(item).sum().item()\n",
    "        batch_correct += corrects\n",
    "    return batch_correct\n",
    "\n",
    "def loss_batch(loss_func_ce, loss_func_ctc, output, valid_len_out, target, opt=None):\n",
    "\n",
    "    #output_prob_ce = output['gloss_probabilities_log'].softmax(dim=1)\n",
    "    #predictions = torch.argmax(output, dim=2)\n",
    "    predictions = torch.transpose(output, 0, 1)\n",
    "\n",
    "    CE_loss = 0  ##n check wat happens when remove\n",
    "    for i, row in enumerate(predictions):\n",
    "        ce_loss = loss_func_ce(row, target)\n",
    "        CE_loss += ce_loss.item()\n",
    "    \n",
    "    #CE_loss = loss_func_ce(predictions, target)\n",
    "    CTC_loss = compute_recognition_loss(loss_func_ctc, target, torch.tensor([1,1,1]), output, valid_len_out) #torch.tensor([1,1,1]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        metric_b = metrics_batch(predictions, target)\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        ce_loss.backward(retain_graph=True)\n",
    "        CTC_loss.backward()\n",
    "        #loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    total_loss = (CE_loss*1 + CTC_loss.item()*3)/4 #+ CE_loss2.item()*1\n",
    "    \n",
    "    return total_loss, metric_b\n",
    "\n",
    "def loss_epoch(model,loss_func_ce,loss_func_ctc,dataset_dl_v,dataset_dl_k,sanity_check=False,opt=None):\n",
    "    running_loss_v=0.0\n",
    "    running_metric_v=0.0\n",
    "    running_wer_v=0.0\n",
    "    running_loss_k=0.0\n",
    "    running_metric_k=0.0\n",
    "    running_wer_k=0.0\n",
    "    len_data = len(dataset_dl_v.dataset)\n",
    "    for (xb_v, yb_v), (xb_k, yb_k) in zip(dataset_dl_v, dataset_dl_k):\n",
    "        xb_v = xb_v.cuda() #.to(device)\n",
    "        x, x_v, x_k, valid_len_out_v, valid_len_out_k = model(xb_v, xb_k.cpu())\n",
    "        \n",
    "        #loss_b, metric_b = loss_batch(loss_func_ce, loss_func_ctc, x, valid_len_out, yb_v, opt)\n",
    "        loss_b_v, metric_b_v = loss_batch(loss_func_ce, loss_func_ctc, x_v, valid_len_out_v, yb_v, opt) #doesn't matter if we pick yb_v or k, should be the same\n",
    "        loss_b_k, metric_b_k = loss_batch(loss_func_ce, loss_func_ctc, x_k, valid_len_out_k, yb_k, opt)\n",
    "        running_loss_v += loss_b_v\n",
    "        running_loss_k += loss_b_k\n",
    "\n",
    "        # GET ACCURACY\n",
    "        if metric_b_v is not None:\n",
    "            running_metric_v+=metric_b_v\n",
    "        if metric_b_v is not None:\n",
    "            running_metric_k+=metric_b_k\n",
    "            \n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "        # GET WORD ERROR RATE\n",
    "        ####\n",
    "\n",
    "    loss_v=running_loss_v/float(len_data)\n",
    "    loss_k=running_loss_k/float(len_data)\n",
    "    metric_v=running_metric_v/float(len_data)\n",
    "    metric_k=running_metric_k/float(len_data)\n",
    "    loss = loss_v + loss_k\n",
    "    metric = metric_v + metric_k\n",
    "    return loss, metric\n",
    "\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def train_val(model, params):\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func_ctc=params[\"loss_func_ctc\"]\n",
    "    loss_func_ce=params[\"loss_func_ce\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl_v=params[\"train_dl_video\"]\n",
    "    train_dl_k=params[\"train_dl_keypoints\"]\n",
    "    val_dl_v=params[\"val_dl_video\"]\n",
    "    val_dl_k=params[\"val_dl_keypoints\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "\n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss=float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model,loss_func_ce,loss_func_ctc,train_dl_v,train_dl_k,sanity_check,opt)\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model,loss_func_ce,loss_func_ctc,val_dl_v,val_dl_k,sanity_check)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights\")\n",
    "\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "\n",
    "        print(\"train loss: %.6f, dev loss: %.6f, accuracy: %.2f\" %(train_loss,val_loss,100*val_metric))\n",
    "        print(\"-\"*10)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f9f74-77f1-4283-9abb-6320704065d6",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5d424f9-217d-49ef-a7bb-14a79c339ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/79, current lr=0.001\n",
      "Copied best model weights\n",
      "train loss: 0.000657, dev loss: 0.005964, accuracy: 0.00\n",
      "----------\n",
      "Epoch 1/79, current lr=0.001\n",
      "train loss: 0.000664, dev loss: 0.006134, accuracy: 0.00\n",
      "----------\n",
      "Epoch 2/79, current lr=0.001\n",
      "train loss: 0.000684, dev loss: 0.006310, accuracy: 0.00\n",
      "----------\n",
      "Epoch 3/79, current lr=0.001\n",
      "train loss: 0.000701, dev loss: 0.006137, accuracy: 0.00\n",
      "----------\n",
      "Epoch 4/79, current lr=0.001\n",
      "Epoch 00005: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000684, dev loss: 0.006032, accuracy: 0.00\n",
      "----------\n",
      "Epoch 5/79, current lr=0.0008\n",
      "Copied best model weights\n",
      "train loss: 0.000664, dev loss: 0.005947, accuracy: 0.00\n",
      "----------\n",
      "Epoch 6/79, current lr=0.0008\n",
      "Copied best model weights\n",
      "train loss: 0.000662, dev loss: 0.005942, accuracy: 0.00\n",
      "----------\n",
      "Epoch 7/79, current lr=0.0008\n",
      "train loss: 0.000661, dev loss: 0.005944, accuracy: 0.00\n",
      "----------\n",
      "Epoch 8/79, current lr=0.0008\n",
      "train loss: 0.000662, dev loss: 0.005952, accuracy: 0.00\n",
      "----------\n",
      "Epoch 9/79, current lr=0.0008\n",
      "train loss: 0.000662, dev loss: 0.005968, accuracy: 0.00\n",
      "----------\n",
      "Epoch 10/79, current lr=0.0008\n",
      "Epoch 00011: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000665, dev loss: 0.005996, accuracy: 0.00\n",
      "----------\n",
      "Epoch 11/79, current lr=0.00064\n",
      "train loss: 0.000661, dev loss: 0.005961, accuracy: 0.00\n",
      "----------\n",
      "Epoch 12/79, current lr=0.00064\n",
      "train loss: 0.000616, dev loss: 0.005990, accuracy: 0.00\n",
      "----------\n",
      "Epoch 13/79, current lr=0.00064\n",
      "train loss: 0.000667, dev loss: 0.006036, accuracy: 0.00\n",
      "----------\n",
      "Epoch 14/79, current lr=0.00064\n",
      "Epoch 00015: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000672, dev loss: 0.006107, accuracy: 0.00\n",
      "----------\n",
      "Epoch 15/79, current lr=0.0005120000000000001\n",
      "train loss: 0.000661, dev loss: 0.005966, accuracy: 0.00\n",
      "----------\n",
      "Epoch 16/79, current lr=0.0005120000000000001\n",
      "train loss: 0.000664, dev loss: 0.006003, accuracy: 0.00\n",
      "----------\n",
      "Epoch 17/79, current lr=0.0005120000000000001\n",
      "train loss: 0.000668, dev loss: 0.006061, accuracy: 0.00\n",
      "----------\n",
      "Epoch 18/79, current lr=0.0005120000000000001\n",
      "Epoch 00019: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000666, dev loss: 0.006148, accuracy: 0.00\n",
      "----------\n",
      "Epoch 19/79, current lr=0.0004096000000000001\n",
      "train loss: 0.000661, dev loss: 0.005967, accuracy: 0.00\n",
      "----------\n",
      "Epoch 20/79, current lr=0.0004096000000000001\n",
      "train loss: 0.000664, dev loss: 0.006004, accuracy: 0.00\n",
      "----------\n",
      "Epoch 21/79, current lr=0.0004096000000000001\n",
      "train loss: 0.000669, dev loss: 0.006059, accuracy: 0.00\n",
      "----------\n",
      "Epoch 22/79, current lr=0.0004096000000000001\n",
      "Epoch 00023: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000675, dev loss: 0.006133, accuracy: 0.00\n",
      "----------\n",
      "Epoch 23/79, current lr=0.0003276800000000001\n",
      "train loss: 0.000661, dev loss: 0.005963, accuracy: 0.00\n",
      "----------\n",
      "Epoch 24/79, current lr=0.0003276800000000001\n",
      "train loss: 0.000663, dev loss: 0.005992, accuracy: 0.00\n",
      "----------\n",
      "Epoch 25/79, current lr=0.0003276800000000001\n",
      "train loss: 0.000667, dev loss: 0.006033, accuracy: 0.00\n",
      "----------\n",
      "Epoch 26/79, current lr=0.0003276800000000001\n",
      "Epoch 00027: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000671, dev loss: 0.006089, accuracy: 0.00\n",
      "----------\n",
      "Epoch 27/79, current lr=0.0002621440000000001\n",
      "train loss: 0.000661, dev loss: 0.005960, accuracy: 0.00\n",
      "----------\n",
      "Epoch 28/79, current lr=0.0002621440000000001\n",
      "train loss: 0.000663, dev loss: 0.005983, accuracy: 0.00\n",
      "----------\n",
      "Epoch 29/79, current lr=0.0002621440000000001\n",
      "train loss: 0.000666, dev loss: 0.006014, accuracy: 0.00\n",
      "----------\n",
      "Epoch 30/79, current lr=0.0002621440000000001\n",
      "Epoch 00031: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000670, dev loss: 0.006054, accuracy: 0.00\n",
      "----------\n",
      "Epoch 31/79, current lr=0.00020971520000000012\n",
      "train loss: 0.000661, dev loss: 0.005956, accuracy: 0.00\n",
      "----------\n",
      "Epoch 32/79, current lr=0.00020971520000000012\n",
      "train loss: 0.000663, dev loss: 0.005974, accuracy: 0.00\n",
      "----------\n",
      "Epoch 33/79, current lr=0.00020971520000000012\n",
      "train loss: 0.000665, dev loss: 0.005997, accuracy: 0.00\n",
      "----------\n",
      "Epoch 34/79, current lr=0.00020971520000000012\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000667, dev loss: 0.006026, accuracy: 0.00\n",
      "----------\n",
      "Epoch 35/79, current lr=0.0001677721600000001\n",
      "train loss: 0.000612, dev loss: 0.005953, accuracy: 0.00\n",
      "----------\n",
      "Epoch 36/79, current lr=0.0001677721600000001\n",
      "train loss: 0.000663, dev loss: 0.005967, accuracy: 0.00\n",
      "----------\n",
      "Epoch 37/79, current lr=0.0001677721600000001\n",
      "train loss: 0.000664, dev loss: 0.005983, accuracy: 0.00\n",
      "----------\n",
      "Epoch 38/79, current lr=0.0001677721600000001\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000667, dev loss: 0.006004, accuracy: 0.00\n",
      "----------\n",
      "Epoch 39/79, current lr=0.00013421772800000008\n",
      "train loss: 0.000661, dev loss: 0.005951, accuracy: 0.00\n",
      "----------\n",
      "Epoch 40/79, current lr=0.00013421772800000008\n",
      "train loss: 0.000662, dev loss: 0.005962, accuracy: 0.00\n",
      "----------\n",
      "Epoch 41/79, current lr=0.00013421772800000008\n",
      "train loss: 0.000663, dev loss: 0.005974, accuracy: 0.00\n",
      "----------\n",
      "Epoch 42/79, current lr=0.00013421772800000008\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Loading best model weights\n",
      "train loss: 0.000665, dev loss: 0.005988, accuracy: 0.00\n",
      "----------\n",
      "Epoch 43/79, current lr=0.00010737418240000007\n",
      "train loss: 0.000661, dev loss: 0.005949, accuracy: 0.00\n",
      "----------\n",
      "Epoch 44/79, current lr=0.00010737418240000007\n",
      "train loss: 0.000661, dev loss: 0.005957, accuracy: 0.00\n",
      "----------\n",
      "Epoch 45/79, current lr=0.00010737418240000007\n",
      "train loss: 0.000663, dev loss: 0.005967, accuracy: 0.00\n",
      "----------\n",
      "Epoch 46/79, current lr=0.00010737418240000007\n",
      "Epoch 00047: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000664, dev loss: 0.005977, accuracy: 0.00\n",
      "----------\n",
      "Epoch 47/79, current lr=8.589934592000007e-05\n",
      "train loss: 0.000661, dev loss: 0.005948, accuracy: 0.00\n",
      "----------\n",
      "Epoch 48/79, current lr=8.589934592000007e-05\n",
      "train loss: 0.000662, dev loss: 0.005954, accuracy: 0.00\n",
      "----------\n",
      "Epoch 49/79, current lr=8.589934592000007e-05\n",
      "train loss: 0.000662, dev loss: 0.005961, accuracy: 0.00\n",
      "----------\n",
      "Epoch 50/79, current lr=8.589934592000007e-05\n",
      "Epoch 00051: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000663, dev loss: 0.005969, accuracy: 0.00\n",
      "----------\n",
      "Epoch 51/79, current lr=6.871947673600006e-05\n",
      "train loss: 0.000661, dev loss: 0.005947, accuracy: 0.00\n",
      "----------\n",
      "Epoch 52/79, current lr=6.871947673600006e-05\n",
      "train loss: 0.000662, dev loss: 0.005952, accuracy: 0.00\n",
      "----------\n",
      "Epoch 53/79, current lr=6.871947673600006e-05\n",
      "train loss: 0.000663, dev loss: 0.005957, accuracy: 0.00\n",
      "----------\n",
      "Epoch 54/79, current lr=6.871947673600006e-05\n",
      "Epoch 00055: reducing learning rate of group 0 to 5.4976e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000663, dev loss: 0.005963, accuracy: 0.00\n",
      "----------\n",
      "Epoch 55/79, current lr=5.497558138880005e-05\n",
      "train loss: 0.000661, dev loss: 0.005946, accuracy: 0.00\n",
      "----------\n",
      "Epoch 56/79, current lr=5.497558138880005e-05\n",
      "train loss: 0.000662, dev loss: 0.005950, accuracy: 0.00\n",
      "----------\n",
      "Epoch 57/79, current lr=5.497558138880005e-05\n",
      "train loss: 0.000662, dev loss: 0.005954, accuracy: 0.00\n",
      "----------\n",
      "Epoch 58/79, current lr=5.497558138880005e-05\n",
      "Epoch 00059: reducing learning rate of group 0 to 4.3980e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000663, dev loss: 0.005958, accuracy: 0.00\n",
      "----------\n",
      "Epoch 59/79, current lr=4.3980465111040044e-05\n",
      "train loss: 0.000661, dev loss: 0.005945, accuracy: 0.00\n",
      "----------\n",
      "Epoch 60/79, current lr=4.3980465111040044e-05\n",
      "train loss: 0.000662, dev loss: 0.005948, accuracy: 0.00\n",
      "----------\n",
      "Epoch 61/79, current lr=4.3980465111040044e-05\n",
      "train loss: 0.000662, dev loss: 0.005951, accuracy: 0.00\n",
      "----------\n",
      "Epoch 62/79, current lr=4.3980465111040044e-05\n",
      "Epoch 00063: reducing learning rate of group 0 to 3.5184e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000663, dev loss: 0.005954, accuracy: 0.00\n",
      "----------\n",
      "Epoch 63/79, current lr=3.5184372088832036e-05\n",
      "train loss: 0.000661, dev loss: 0.005945, accuracy: 0.00\n",
      "----------\n",
      "Epoch 64/79, current lr=3.5184372088832036e-05\n",
      "train loss: 0.000662, dev loss: 0.005947, accuracy: 0.00\n",
      "----------\n",
      "Epoch 65/79, current lr=3.5184372088832036e-05\n",
      "train loss: 0.000662, dev loss: 0.005949, accuracy: 0.00\n",
      "----------\n",
      "Epoch 66/79, current lr=3.5184372088832036e-05\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.8147e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000662, dev loss: 0.005952, accuracy: 0.00\n",
      "----------\n",
      "Epoch 67/79, current lr=2.814749767106563e-05\n",
      "train loss: 0.000661, dev loss: 0.005944, accuracy: 0.00\n",
      "----------\n",
      "Epoch 68/79, current lr=2.814749767106563e-05\n",
      "train loss: 0.000661, dev loss: 0.005946, accuracy: 0.00\n",
      "----------\n",
      "Epoch 69/79, current lr=2.814749767106563e-05\n",
      "train loss: 0.000661, dev loss: 0.005948, accuracy: 0.00\n",
      "----------\n",
      "Epoch 70/79, current lr=2.814749767106563e-05\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.2518e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000662, dev loss: 0.005950, accuracy: 0.00\n",
      "----------\n",
      "Epoch 71/79, current lr=2.2517998136852506e-05\n",
      "train loss: 0.000662, dev loss: 0.005944, accuracy: 0.00\n",
      "----------\n",
      "Epoch 72/79, current lr=2.2517998136852506e-05\n",
      "train loss: 0.000662, dev loss: 0.005945, accuracy: 0.00\n",
      "----------\n",
      "Epoch 73/79, current lr=2.2517998136852506e-05\n",
      "train loss: 0.000662, dev loss: 0.005947, accuracy: 0.00\n",
      "----------\n",
      "Epoch 74/79, current lr=2.2517998136852506e-05\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.8014e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000662, dev loss: 0.005948, accuracy: 0.00\n",
      "----------\n",
      "Epoch 75/79, current lr=1.8014398509482006e-05\n",
      "train loss: 0.000661, dev loss: 0.005943, accuracy: 0.00\n",
      "----------\n",
      "Epoch 76/79, current lr=1.8014398509482006e-05\n",
      "train loss: 0.000661, dev loss: 0.005945, accuracy: 0.00\n",
      "----------\n",
      "Epoch 77/79, current lr=1.8014398509482006e-05\n",
      "train loss: 0.000661, dev loss: 0.005946, accuracy: 0.00\n",
      "----------\n",
      "Epoch 78/79, current lr=1.8014398509482006e-05\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.4412e-05.\n",
      "Loading best model weights\n",
      "train loss: 0.000661, dev loss: 0.005947, accuracy: 0.00\n",
      "----------\n",
      "Epoch 79/79, current lr=1.4411518807585605e-05\n",
      "train loss: 0.000661, dev loss: 0.005943, accuracy: 0.00\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import tqdm\n",
    "\n",
    "#model_type=\"S3DandVisualHeadTest\"\n",
    "model_type=\"VisualHeadKeypointTest\"\n",
    "\n",
    "loss_func_ctc = torch.nn.CTCLoss(zero_infinity=True, reduction='sum')\n",
    "loss_func_ce = torch.nn.CrossEntropyLoss(reduction='sum', label_smoothing=0.1)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)        # original lr=3e-5 in baseline paper lr=1e-3. Baseline also uses adam with batch size of 8 and weight decay of 1e-3.\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.8, patience=3, verbose=True, groups=2??)    # patientce is 5, so after 5 epochs of no improvement learning rate is lr*factor, so halved\n",
    "#lr_scheduler = CosineAnnealingLR(opt, T_max=40)     # maximum amount of iterations on the lr\n",
    "os.makedirs(\"Thesis/Models\", exist_ok=True)\n",
    "\n",
    "params_train={\n",
    "    \"num_epochs\": 80,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func_ctc\": loss_func_ctc,\n",
    "    \"loss_func_ce\": loss_func_ce,\n",
    "    \"train_dl_keypoints\": train_dl_keypoints,\n",
    "    \"train_dl_video\": train_dl_video,\n",
    "    \"val_dl_keypoints\": test_dl_keypoints,\n",
    "    \"val_dl_video\": test_dl_video,\n",
    "    \"sanity_check\": True,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": \"Models/weights_\"+model_type+\".pt\",\n",
    "    }\n",
    "model,loss_hist,metric_hist = train_val(model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dea7f86a-694d-4b75-9f18-eba8a83a7c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss_hist, metric_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8b7ff-c250-4221-9f92-aa6dad942442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
