{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_GA-uB7Wj2d"
   },
   "source": [
    "TO DO: Turn test data into pose data and save with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-mu4VS92xh2",
    "outputId": "3d71e0d3-856b-4088-c842-510f07b64b23"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rriNU-wq_CUG",
    "outputId": "523e23ed-a90f-4218-fef9-e0c2d041452a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\novan\\anaconda3\\lib\\site-packages (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\novan\\appdata\\roaming\\python\\python38\\site-packages (0.10.11)\n",
      "Requirement already satisfied: absl-py in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (2.0)\n",
      "Requirement already satisfied: jax in c:\\users\\novan\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (3.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\novan\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\novan\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\novan\\appdata\\roaming\\python\\python38\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\novan\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from jax->mediapipe) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2024.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\novan\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\novan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user opencv-python\n",
    "!pip install --user mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediapipe Data Preprocessing\n",
    "\n",
    "### Keypoint Selection\n",
    "\n",
    "We have to make a selection of which keypoints we choose for the model, since taking all keypoints would mean more than 600 keypoints of which 500 are of the face. So we need to select! In the signgraph paper from https://ieeexplore.ieee.org/abstract/document/10049842 they have this to say:\n",
    "\n",
    "\"This model generated a total of 540+ landmarks, out of\n",
    "which we have used data for only 65 landmarks. These\n",
    "65 landmarks consist of pose information for both hands,\n",
    "arms, body torso and some significant facial nodes like eyes,\n",
    "nose, ears, and lips. We have discarded all the remaining land-\n",
    "marks because they were providing no additional information\n",
    "in our model.\"\n",
    "\n",
    "So this is what we aim for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.19\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "\n",
    "#!pip install --user opencv-python\n",
    "#!pip install --user mediapipe\n",
    "\n",
    "# First import and initialize everything needed\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic     #There are many different models you can use, like just face, hand or pose detections!\n",
    "\n",
    "gloss_path = 'Data/CorpusNGT/gloss_split_8frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noseTip\n",
      "96\n",
      "46\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# Found online a list of all the face mesh keypoints and their corresponding numbers so we are\n",
    "# able to pick the useful ones.\n",
    "\n",
    "from mediapipe.python.solutions.pose import PoseLandmark\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_CONTOURS\n",
    "from mediapipe.python.solutions.face_mesh_connections import FACEMESH_TESSELATION\n",
    "\n",
    "MESH_ANNOTATIONS = {\n",
    "  \"silhouette\": [\n",
    "    10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\n",
    "    397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\n",
    "    172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\n",
    "  ],\n",
    "\n",
    "  \"lipsUpperOuter\": [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291], #keep this for mouth shape\n",
    "  \"lipsLowerOuter\": [146, 91, 181, 84, 17, 314, 405, 321, 375, 291], #keep this for mouth shape\n",
    "  \"lipsUpperInner\": [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308], #keep this for mouth shape\n",
    "  \"lipsLowerInner\": [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308], #keep this for mouth shape\n",
    "\n",
    "  \"rightEyeUpper0\": [246, 161, 160, 159, 158, 157, 173], #keep this for eye location and opening\n",
    "  \"rightEyeLower0\": [33, 7, 163, 144, 145, 153, 154, 155, 133], #keep this for eye location and opening\n",
    "  \"rightEyeUpper1\": [247, 30, 29, 27, 28, 56, 190],\n",
    "  \"rightEyeLower1\": [130, 25, 110, 24, 23, 22, 26, 112, 243],\n",
    "  \"rightEyeUpper2\": [113, 225, 224, 223, 222, 221, 189],\n",
    "  \"rightEyeLower2\": [226, 31, 228, 229, 230, 231, 232, 233, 244],\n",
    "  \"rightEyeLower3\": [143, 111, 117, 118, 119, 120, 121, 128, 245],\n",
    "\n",
    "  \"rightEyebrowUpper\": [156, 70, 63, 105, 66, 107, 55, 193], #keep this for eyebrow location and raising\n",
    "  \"rightEyebrowLower\": [35, 124, 46, 53, 52, 65], #keep this for eyebrow location and raising\n",
    "\n",
    "  \"rightEyeIris\": [473, 474, 475, 476, 477], #keep this for eye direction\n",
    "\n",
    "  \"leftEyeUpper0\": [466, 388, 387, 386, 385, 384, 398], #keep this for eye location and opening\n",
    "  \"leftEyeLower0\": [263, 249, 390, 373, 374, 380, 381, 382, 362], #keep this for eye location and opening\n",
    "  \"leftEyeUpper1\": [467, 260, 259, 257, 258, 286, 414],\n",
    "  \"leftEyeLower1\": [359, 255, 339, 254, 253, 252, 256, 341, 463],\n",
    "  \"leftEyeUpper2\": [342, 445, 444, 443, 442, 441, 413],\n",
    "  \"leftEyeLower2\": [446, 261, 448, 449, 450, 451, 452, 453, 464],\n",
    "  \"leftEyeLower3\": [372, 340, 346, 347, 348, 349, 350, 357, 465],\n",
    "\n",
    "  \"leftEyebrowUpper\": [383, 300, 293, 334, 296, 336, 285, 417], #keep this for eyebrow location and raising\n",
    "  \"leftEyebrowLower\": [265, 353, 276, 283, 282, 295], #keep this for eyebrow location and raising\n",
    "\n",
    "  \"leftEyeIris\": [468, 469, 470, 471, 472], #keep this for eye direction\n",
    "\n",
    "  \"midwayBetweenEyes\": [168],\n",
    "\n",
    "  \"noseTip\": [1], #keep this for nose location\n",
    "  \"noseBottom\": [2], #keep this for nose location\n",
    "  \"noseRightCorner\": [98], #keep this for nose location\n",
    "  \"noseLeftCorner\": [327], #keep this for nose location\n",
    "\n",
    "  \"rightCheek\": [205],\n",
    "  \"leftCheek\": [425]\n",
    "};\n",
    "\n",
    "face_mesh_annotations_to_use = {\n",
    "    61: \"lipsUpperOuter1\",\n",
    "    37: \"lipsUpperOuter2\",\n",
    "    267: \"lipsUpperOuter3\",\n",
    "    291: \"lipsUpperOuter4\",\n",
    "    146: \"lipsLowerOuter1\",\n",
    "    181: \"lipsLowerOuter2\",\n",
    "    17: \"lipsLowerOuter3\",\n",
    "    405: \"lipsLowerOuter4\",\n",
    "    375: \"lipsLowerOuter5\",\n",
    "    78: \"lipsUpperInner1\",\n",
    "    81: \"lipsUpperInner2\",\n",
    "    311: \"lipsUpperInner3\",\n",
    "    308: \"lipsUpperInner4\",\n",
    "    88: \"lipsLowerInner1\",\n",
    "    87: \"lipsLowerInner2\",\n",
    "    317: \"lipsLowerInner3\",\n",
    "    318: \"lipsLowerInner4\",\n",
    "    246: \"rightEyeUpper1\",\n",
    "    160: \"rightEyeUpper2\",\n",
    "    158: \"rightEyeUpper3\",\n",
    "    173: \"rightEyeUpper4\",\n",
    "    163: \"rightEyeLower1\",\n",
    "    145: \"rightEyeLower2\",\n",
    "    154: \"rightEyeLower3\",\n",
    "    70: \"rightEyebrowUpper1\",\n",
    "    105: \"rightEyebrowUpper2\",\n",
    "    107: \"rightEyebrowUpper3\",\n",
    "    46: \"rightEyebrowLower1\",\n",
    "    52: \"rightEyebrowLower2\",\n",
    "    55: \"rightEyebrowLower3\",\n",
    "    473: \"rightEyeIris\",\n",
    "    466: \"leftEyeUpper1\",\n",
    "    387: \"leftEyeUpper2\",\n",
    "    385: \"leftEyeUpper3\",\n",
    "    398: \"leftEyeUpper4\",\n",
    "    390: \"leftEyeLower1\",\n",
    "    374: \"leftEyeLower2\",\n",
    "    381: \"leftEyeLower3\",\n",
    "    300: \"leftEyebrowUpper1\",\n",
    "    334: \"leftEyebrowUpper2\",\n",
    "    336: \"leftEyebrowUpper3\",\n",
    "    276: \"leftEyebrowLower1\",\n",
    "    282: \"leftEyebrowLower2\",\n",
    "    285: \"leftEyebrowLower3\",\n",
    "    468: \"leftEyeIris\",\n",
    "    4: \"noseTip\"\n",
    "}\n",
    "\n",
    "print(face_mesh_annotations_to_use[4])\n",
    "\n",
    "# list of landmarks to exclude from the drawing\n",
    "excluded_landmarks = [\n",
    "    PoseLandmark.LEFT_EYE, \n",
    "    PoseLandmark.RIGHT_EYE, \n",
    "    PoseLandmark.LEFT_EYE_INNER, \n",
    "    PoseLandmark.RIGHT_EYE_INNER, \n",
    "    PoseLandmark.LEFT_EAR,\n",
    "    PoseLandmark.RIGHT_EAR,\n",
    "    PoseLandmark.LEFT_EYE_OUTER,\n",
    "    PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    PoseLandmark.NOSE,\n",
    "    PoseLandmark.MOUTH_LEFT,\n",
    "    PoseLandmark.MOUTH_RIGHT,\n",
    "    PoseLandmark.LEFT_KNEE,\n",
    "    PoseLandmark.RIGHT_KNEE,\n",
    "    PoseLandmark.LEFT_ANKLE,\n",
    "    PoseLandmark.RIGHT_ANKLE,\n",
    "    PoseLandmark.LEFT_HEEL,\n",
    "    PoseLandmark.RIGHT_HEEL,\n",
    "    PoseLandmark.LEFT_FOOT_INDEX,\n",
    "    PoseLandmark.RIGHT_FOOT_INDEX,\n",
    "    PoseLandmark.LEFT_PINKY,\n",
    "    PoseLandmark.RIGHT_PINKY,\n",
    "    PoseLandmark.LEFT_INDEX,\n",
    "    PoseLandmark.RIGHT_INDEX,\n",
    "    PoseLandmark.LEFT_THUMB,\n",
    "    PoseLandmark.RIGHT_THUMB]\n",
    "\n",
    "pose_landmarks_to_use = [\n",
    "    PoseLandmark.LEFT_SHOULDER,\n",
    "    PoseLandmark.RIGHT_SHOULDER,\n",
    "    PoseLandmark.LEFT_ELBOW,\n",
    "    PoseLandmark.RIGHT_ELBOW,\n",
    "    PoseLandmark.LEFT_WRIST,\n",
    "    PoseLandmark.RIGHT_WRIST,\n",
    "    PoseLandmark.LEFT_HIP,\n",
    "    PoseLandmark.RIGHT_HIP]\n",
    "\n",
    "# For hands we keep all of the keypoints\n",
    "face_mesh_keeplist = [61, 37, 267, 291, 146, 181, 17, 405, 375, 78, 81, 311, 308, 88, 87, 317, 318, \n",
    "                      246, 160, 158, 173, 163, 145, 154, 70, 105, 107, 46, 52, 55, 473, \n",
    "                     466, 387, 385, 398, 390, 374, 381, 300, 334, 336, 276, 282, 285, 468, 4]\n",
    "pose_keeplist = [11, 12, 13, 14, 15, 16, 23, 24]\n",
    "hands_keeplist = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "print(len(face_mesh_keeplist)+len(pose_keeplist)+len(hands_keeplist)+len(hands_keeplist))\n",
    "print(len(face_mesh_keeplist))\n",
    "print(len(hands_keeplist))\n",
    "# We only want to keep these keypoints so we make a list of them so we can check the results against this list later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on one video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test on one video to make sure pose estimation is working properly. The pose estimation will start up in another window, \n",
    "## check if the model is estimating properly for every frame.\n",
    "\n",
    "# Initialize MP pose.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, # Makes the model treat the input as a video\n",
    "                    model_complexity=2,\n",
    "                    enable_segmentation=True,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    refine_face_landmarks=True)\n",
    "\n",
    "custom_style = mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "custom_connections = list(mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "for landmark in excluded_landmarks:\n",
    "    # we change the way the excluded landmarks are drawn\n",
    "    custom_style[landmark] = DrawingSpec(color=(255,255,255), thickness=0) \n",
    "    # we remove all connections which contain these landmarks\n",
    "    custom_connections = [connection_tuple for connection_tuple in custom_connections \n",
    "                            if landmark.value not in connection_tuple]\n",
    "\n",
    "#cap = cv2.VideoCapture(gloss_path + \"/ONTSLAAN-A/0.mpg\")\n",
    "cap = cv2.VideoCapture(\"Data/CorpusNGT/gloss/CNGT0004_S004.mpg\")\n",
    "#cap = cv2.VideoCapture(0)  ## Video stream from camera\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Video or stream has ended. Exiting ...\")\n",
    "        break\n",
    "    else:\n",
    "        #print(\"Frame read success\")\n",
    "        # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image)\n",
    "    \n",
    "        # Draw landmark annotation on the image.\n",
    "        mp_drawing.draw_landmarks(image,\n",
    "                            results.pose_landmarks,\n",
    "                            connections = custom_connections, #  passing the modified connections list\n",
    "                            landmark_drawing_spec=custom_style) # and drawing style \n",
    "        mp_drawing.draw_landmarks(image,\n",
    "                            results.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_CONTOURS,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "        mp_drawing.draw_landmarks(image,\n",
    "                            results.left_hand_landmarks,\n",
    "                            mp_holistic.HAND_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1),\n",
    "                            connection_drawing_spec=mp_drawing.DrawingSpec(color=(200, 200, 200), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image,\n",
    "                            results.right_hand_landmarks,\n",
    "                            mp_holistic.HAND_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1),\n",
    "                            connection_drawing_spec=mp_drawing.DrawingSpec(color=(200, 200, 200), thickness=1, circle_radius=1))\n",
    "\n",
    "        # Naming a window \n",
    "        cv2.namedWindow(\"Resized_Window\", cv2.WINDOW_NORMAL) \n",
    "        cv2.resizeWindow(\"Resized_Window\", 800, 600) \n",
    "\n",
    "        # Display frame\n",
    "        cv2.imshow(\"Resized_Window\", image)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If testing is successful, run the code to estimate pose for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P0J7WUPZClT",
    "outputId": "2a5c110e-ea04-4017-c8e4-448415fab784",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = np.load('Data/CorpusNGT/gloss_labels.npy', allow_pickle='TRUE').item()\n",
    "gloss_count = np.load('Data/CorpusNGT/gloss_counts.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MP pose.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, # Makes the model treat the input as a video\n",
    "                    model_complexity=2,\n",
    "                    enable_segmentation=True,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    refine_face_landmarks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3780\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(gloss_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3780/3780 [03:17<00:00, 19.15it/s]\n"
     ]
    }
   ],
   "source": [
    "## DEBUGGING\n",
    "\n",
    "## Remove previous keypoints\n",
    "for catg in tqdm.tqdm(os.listdir(gloss_path)):\n",
    "    f1 = '{}/{}'.format(gloss_path, catg)\n",
    "    for number in os.listdir(f1):\n",
    "        f2 = '{}/{}'.format(f1, number)\n",
    "        if os.path.isdir(f2):\n",
    "            if \"frame8.jpg\" in os.listdir(f2):\n",
    "                os.remove('{}/{}'.format(f2, \"frame8.jpg\"))  ## or \"keypoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3780/3780 [30:26:05<00:00, 28.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Perform pose estimation on all frames and save to dataframe\n",
    "\n",
    "# This model generated a total of 540+ landmarks, out of which we have used data for only 65 landmarks. These 65 landmarks consist of \n",
    "# pose information for both hands, arms, body torso and some significant facial nodes like eyes, nose, ears, and lips.\n",
    "\n",
    "# Only run this once! if you break off the code here halfway and run it again it will not work.\n",
    "\n",
    "def CreateKeypoint(Model, frame_idx, idx, landmark_name, x, y):\n",
    "    keypoint_entry = {\n",
    "        'frame': frame_idx,\n",
    "        'landmark model': Model,\n",
    "        'landmark index': idx,\n",
    "        'landmark name': landmark_name,\n",
    "        'x': x,\n",
    "        'y': y\n",
    "    }\n",
    "    return keypoint_entry\n",
    "\n",
    "def ExtractKeypoints(results, mp_pose_type, whichlandmarkstouse, frame_idx, Model=\"Face\"):\n",
    "    # If any pose is detected\n",
    "    frame_keypoints = []\n",
    "    if results: #if the pose estimation was successful\n",
    "        for idx, landmark in enumerate(results.landmark):\n",
    "            if idx in whichlandmarkstouse:\n",
    "                frame_keypoints.append(CreateKeypoint(Model, frame_idx, idx, mp_pose_type[idx] if Model==\"Face\" else mp_pose_type(idx).name, landmark.x, landmark.y))\n",
    "    else: #if the model did not estimate properly we fill in zeroes.\n",
    "        if Model==\"Face\":\n",
    "            for (_, idx) in enumerate(mp_pose_type):\n",
    "                frame_keypoints.append(CreateKeypoint(Model, frame_idx, idx, mp_pose_type[idx], 0, 0))\n",
    "        elif Model==\"LeftHand\" or Model==\"RightHand\" or Model==\"Pose\":\n",
    "            for idx in enumerate(mp_pose_type):\n",
    "                frame_keypoints.append(CreateKeypoint(Model, frame_idx, idx[0], mp_pose_type(idx[0]).name, 0, 0))\n",
    "    return frame_keypoints\n",
    "\n",
    "for catg in tqdm.tqdm(os.listdir(gloss_path)):\n",
    "    f1 = '{}/{}'.format(gloss_path, catg)\n",
    "    for number in os.listdir(f1):\n",
    "        f2 = '{}/{}'.format(f1, number)\n",
    "        if os.path.isdir(f2):\n",
    "            allFramesLandmarks = pd.DataFrame()\n",
    "            frame_idx = 0\n",
    "            if \"keypoints\" in os.listdir(f2):\n",
    "                continue\n",
    "            else:\n",
    "                for frame in os.listdir(f2):\n",
    "#                try:\n",
    "                    f3 = '{}/{}'.format(f2, frame)\n",
    "                    split = f3.split(\".\")\n",
    "                    if split[1] == \"jpg\":\n",
    "                        # Read image\n",
    "                        img = cv2.imread(f3)\n",
    "                        \n",
    "                        # Process.\n",
    "                        results = holistic.process(img)\n",
    "    \n",
    "                        landmark_list_pose = pd.DataFrame(ExtractKeypoints(\n",
    "                            results.pose_landmarks, mp_holistic.PoseLandmark, pose_keeplist, frame_idx, Model=\"Pose\"))\n",
    "                        landmark_list_left = pd.DataFrame(ExtractKeypoints(\n",
    "                            results.left_hand_landmarks, mp_holistic.HandLandmark, hands_keeplist, frame_idx, Model=\"LeftHand\"))\n",
    "                        landmark_list_right = pd.DataFrame(ExtractKeypoints(\n",
    "                            results.right_hand_landmarks, mp_holistic.HandLandmark, hands_keeplist, frame_idx, Model=\"RightHand\"))\n",
    "                        landmark_list_face = pd.DataFrame(ExtractKeypoints(\n",
    "                            results.face_landmarks, face_mesh_annotations_to_use, face_mesh_keeplist, frame_idx, Model=\"Face\"))\n",
    "    \n",
    "                        allFramesLandmarks = pd.concat(\n",
    "                            [allFramesLandmarks, landmark_list_pose, landmark_list_left, landmark_list_right, landmark_list_face], ignore_index=True)\n",
    "                        allFramesLandmarks.sort_values(by=['frame', 'landmark model', 'landmark index'], ascending=True, inplace=True)\n",
    "                        allFramesLandmarks.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#                except Exception as err:\n",
    "#                    print(Exception, err)\n",
    "#                    print(f2)\n",
    "                    \n",
    "                    frame_idx += 1\n",
    "            \n",
    "            allFramesLandmarks.to_pickle('{}/{}'.format(f2, \"keypoints\"), compression='infer', protocol=5, storage_options=None)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX_FINGER_MCP\n"
     ]
    }
   ],
   "source": [
    "#for idx in enumerate(mp_holistic.HandLandmark):\n",
    "#    print(idx[1])\n",
    "\n",
    "print(mp_holistic.HandLandmark(5).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(frame_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you successfully ran the mediapipe pose estimation\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list --format=freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
